{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 归一化 normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 归一化的目的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE损失函数是凸函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Img/10.png\" width=\"40%\" height=\"40%\">   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果样本的值X1 << X2，那么最终θ1 >> θ2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "θ1和θ2数值在一开始初始化是差不多的，但是由于X1和X2的关系，我们会发现θ1从初始值到目标位置的距离要远大于θ2从初始值到目标位置，这也是第二幅图扁平的原因"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每次调整θ1的幅度要远小于θ2的幅度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结：归一化的目的是使得最终梯度下降的时候可以不同维度θ参数可以在接近的调整幅度上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "扩展：归一化可以提高精度，主要在距离计算上，样本之间的数量级不同会导致偏向性，所以归一化后就没这个问题了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 最大值最小值归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min max scaling，针对每一列，每一列取出最大最小值，然后对这一列中每个数值按公式操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ X_{\\text{norm}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $X$ 是原始数据点。\n",
    "* $X_{min}$ 是数据集中的最小值。\n",
    "* $X_{max}$ 是数据集中的最大值。\n",
    "* $X_{norm}$ 是归一化后的数据点。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优点是一定可以把数值归一到0到1之间，缺点是如果有一个离群值，影响会比较大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 标准归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也是按照列来做的，求出每一列的均值和标准差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标准归一化包含了均值归一化和方差归一化，经过处理的函数符合标准正态分布，即均值为0，标准差为1，其转化函数为： \n",
    "$$\n",
    "X_{\\text{norm}} = \\frac{X - \\mu}{\\sigma}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方差归一化：标准差的计算会考虑到所有的样本数据，所以受到离群值的影响会小了一些。  \n",
    "均值归一化：当我们梯度下降的时候，能够有更多的方向可以调整"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据不一定会缩放到0到1之间了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [5],\n",
       "       [5]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "temp = np.array([1,2,3,5,5])\n",
    "temp = temp.reshape(-1,1)\n",
    "temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(temp) # fit 方法计算数据的均值和标准差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.56])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.var_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.375],\n",
       "       [-0.75 ],\n",
       "       [-0.125],\n",
       "       [ 1.125],\n",
       "       [ 1.125]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.transform(temp) # 对数据进行标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([1,2,3,5,50001]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    1],\n",
       "       [    2],\n",
       "       [    3],\n",
       "       [    5],\n",
       "       [50001]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10002.4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.99972002e+08])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.var_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5000875 ],\n",
       "       [-0.5000375 ],\n",
       "       [-0.4999875 ],\n",
       "       [-0.49988749],\n",
       "       [ 2.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 强调"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在做特征工程的时候，对训练集做归一化后，测试集也同样需要做归一化，但是它的均值和方差要使用训练集的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 正则化 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 过拟合和欠拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）under fit：还没有拟合到位，训练集和测试集的准确率都还没达到最高。学的还不到位。  \n",
    "（2）over fit：拟合过度，训练集的准确率升高的同时，测试集的准确率反而降低。学的过度了，做过的卷子都能再次答对，考试碰到新题就不会了   \n",
    "（3）just right：过拟合前训练集和测试集准确率都达到最高时刻。学习并不需要花费很多时间，理解的很好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Img/11.png\" width=\"40%\" height=\"40%\">   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正则化就是防止过拟合，具有更好的鲁棒性（robust），也就是模型的泛化能力和推广能力更强。想要有一定的容错率又要保证正确率就要由正则项来决定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正则化的本质就是牺牲模型在训练集上的正确率来提高推广能力，W在数值上越小越好，这样能抵抗数值的扰动。同时为了保证模型的正确率，W又不能极小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此在损失函数的后面加上一个惩罚项，使得计算出来的模型W相对小一些，带来泛化能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Loss = MSE + 正则项$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原来的损失函数是保证MSE最小，找到使得训练集正确率最高的参数。但是我们需要的是测试集上的准确率越高，因此MSE和正则项都得小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（回归的损失函数一般是MSE，分类的话一般是cross entropy交叉熵）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 常用的惩罚项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实L1和L2正则的公式数学里面的意义就是范数，代表空间中向量到原点的距离，W整体越小越好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### L1正则项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数距离原点的曼哈顿距离"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{i=1}^{n} |w_i|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在线性回归中，包含L1正则项的损失函数可以是：\n",
    "$$\\frac{1}{2m} \\sum_{j=1}^{m} (y_j - \\hat{y_j})^2 + \\lambda \\sum_{i=1}^{n} |w_i|$$\n",
    "其中$ \\lambda $是正则化系数，控制惩罚项的权重，m表示样本数量  \n",
    "这个就是Lasso回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.85696912]\n",
      "[5.05414109]\n",
      "[2.53521868]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "x = 2*np.random.rand(100,1)\n",
    "y = 4 + 3*x + np.random.rand(100,1)\n",
    "\n",
    "# alpha 是正则化强度的超参数，用于控制 L2 正则化项的权重\n",
    "# max_iter 是最大迭代次数\n",
    "# alpha调大会更关注泛化能力，会牺牲一些准确率\n",
    "ridge_reg = Lasso(alpha=0.15,max_iter=30000)\n",
    "ridge_reg.fit(x,y)\n",
    "print(ridge_reg.predict([[1.5]]))\n",
    "\n",
    "# 打印模型的截距和系数\n",
    "print(ridge_reg.intercept_)\n",
    "print(ridge_reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### L2正则项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数距离原点的欧式距离"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{i=1}^{n} w_i^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在线性回归中，包含L2正则项的损失函数可以是：\n",
    "$$\\frac{1}{2m} \\sum_{j=1}^{m} (y_j - \\hat{y_j})^2 + \\frac{\\lambda}{2} \\sum_{i=1}^{n} w_i^2$$\n",
    "这个就是Ridge岭回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.90842188]]\n",
      "[4.60162269]\n",
      "[[2.87119946]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "x = 2*np.random.rand(100,1)\n",
    "y = 4 + 3*x + np.random.rand(100,1)\n",
    "\n",
    "# alpha 是正则化强度的超参数，用于控制 L2 正则化项的权重\n",
    "# solver 参数指定了用于优化的算法。sag'：使用随机平均梯度下降法\n",
    "# alpha调大会更关注泛化能力，会牺牲一些准确率\n",
    "ridge_reg = Ridge(alpha=0.0004,solver='sag')\n",
    "ridge_reg.fit(x,y)\n",
    "print(ridge_reg.predict([[1.5]]))\n",
    "\n",
    "# 打印模型的截距和系数\n",
    "print(ridge_reg.intercept_)\n",
    "print(ridge_reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### L1稀疏L2平滑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1正则会使得计算出来的模型有的W趋近于0，有的W相对较大，而L2会使得W参数整体变小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到菱形的交点更多可能在坐标轴上，会导致一些参数值为0，这也是为什么L1稀疏的原因"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Img/12.png\" width=\"40%\" height=\"40%\">   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事实上，我们使用了新的损失函数后，我们的目标不在是原来的照趋近于圆心的点，而是变成了找两个图形的交点，找出哪个满足总损失函数最小的那个交点就是我们想要的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ElasticNet回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ ElasticNet 损失函数 = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( y_i - \\hat{y_i} \\right)^2 + \\alpha \\left( \\frac{1 - \\text{l1\\_ratio}}{2} \\sum_{j=1}^{n} w_j^2 + \\text{l1\\_ratio} \\sum_{j=1}^{n} |w_j| \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "α 是正则化强度的超参数。  \n",
    "l1_ratio 是 L1 正则项的权重比率，介于 0 和 1 之间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.76029894]\n",
      "[5.1218699]\n",
      "[2.42561936]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "x = 2*np.random.rand(100,1)\n",
    "y = 4 + 3*x + np.random.rand(100,1)\n",
    "\n",
    "# alpha 是正则化强度的超参数，用于控制 L2 正则化项的权重\n",
    "# solver 参数指定了用于优化的算法。sag'：使用随机平均梯度下降法\n",
    "# alpha调大会更关注泛化能力，会牺牲一些准确率\n",
    "ridge_reg = ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=10000)\n",
    "ridge_reg.fit(x,y)\n",
    "print(ridge_reg.predict([[1.5]]))\n",
    "\n",
    "# 打印模型的截距和系数\n",
    "print(ridge_reg.intercept_)\n",
    "print(ridge_reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 升维  polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "升维的目的是解决欠拟合的问题，也可以提高模型的准确率，因为维度不够时，也就是预测结果考虑的因素少的话，肯定不能计算出准确的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "升维的最常见手段是将已知维度进行相乘来构建新的维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多项式回归是升维的一种，实现的是Data非线性-->Data线性的过程  \n",
    "例如，原有的数据集有两个维度x1,x2，那么使用多元线性回归公式就是  \n",
    "$\\hat{y} = w_0 + w_1x_1 + w_2x_2$  \n",
    "当我们使用二阶多项式升维的时候，数据集就从原来的x1,x2扩展成了x1,x2,x1^2,x2^2,x1x2，因此多元线性回归就得多计算三个维度所对应的W值，  \n",
    "$\\hat{y} = w_0 + w_1x_1 + w_2x_2 + w_3x_1^2 + w_4x_2^2 + w_5x_1x_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.random.seed(40)\n",
    "m=100\n",
    "x=6*np.random.rand(m,1) - 3\n",
    "y=0.5*x**2 + x + 2 + np.random.randn(m,1)\n",
    "plt.plot(x,y,'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x[:80]\n",
    "y_train = y[:80]\n",
    "x_test = x[80:]\n",
    "y_test = y[80:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.55387783]\n",
      "[ 1.         -0.55387783]\n",
      "(80, 1)\n",
      "(80, 2)\n",
      "0.0 [[3.47963294 0.90900589]]\n",
      "2.8465090916990134\n",
      "1.6413296587819022\n",
      "[-0.55387783]\n",
      "[ 1.         -0.55387783  0.30678065]\n",
      "(80, 1)\n",
      "(80, 3)\n",
      "0.0 [[1.94693516 0.95916707 0.48308844]]\n",
      "1.0219608664434772\n",
      "0.9072838770832732\n",
      "[-0.55387783]\n",
      "[ 1.         -0.55387783  0.30678065 -0.169919    0.09411437 -0.05212786\n",
      "  0.02887247 -0.01599182  0.00885751 -0.00490598  0.00271731]\n",
      "(80, 1)\n",
      "(80, 11)\n",
      "0.0 [[ 1.48380909e+00  8.16401416e-01  2.13409401e+00  7.61517798e-01\n",
      "  -9.58943688e-01 -4.28492482e-01  1.88330342e-01  7.46991218e-02\n",
      "  -1.30996093e-02 -4.06988087e-03  1.79767968e-04]]\n",
      "0.8687404790416766\n",
      "1.1973913367066475\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# d 字典中的键表示多项式特征转换的阶数，1阶，2阶和10阶\n",
    "d = {1: 'g-', 2: 'r+',10: 'y*'}\n",
    "for i in d:\n",
    "    # degree=i 指定了多项式的阶数为当前循环的阶数 i，include_bias=True 表示是否包含截距项\n",
    "    poly_features = PolynomialFeatures(degree=i,include_bias=True)\n",
    "    # 对训练集 x_train 和测试集 x_test 进行多项式特征转换\n",
    "    x_poly_train = poly_features.fit_transform(x_train)\n",
    "    x_poly_test = poly_features.fit_transform(x_test)\n",
    "    print(x_train[0])\n",
    "    print(x_poly_train[0])\n",
    "    print(x_train.shape)\n",
    "    print(x_poly_train.shape)\n",
    "\n",
    "    lin_reg = LinearRegression(fit_intercept=False)\n",
    "    lin_reg.fit(x_poly_train,y_train)\n",
    "    print(lin_reg.intercept_,lin_reg.coef_)\n",
    "\n",
    "    # 看看是否随着degree的增加升维，是否过拟合了\n",
    "    y_train_predict = lin_reg.predict(x_poly_train)\n",
    "    y_test_predict = lin_reg.predict(x_poly_test)\n",
    "\n",
    "    plt.plot(x_poly_train[:,1],y_train_predict,d[i])\n",
    "\n",
    "    print(mean_squared_error(y_train,y_train_predict))\n",
    "    print(mean_squared_error(y_test,y_test_predict))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Opencv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
