{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 归一化 normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 归一化的目的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE损失函数是凸函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Img/10.png\" width=\"40%\" height=\"40%\">   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果样本的值X1 << X2，那么最终θ1 >> θ2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "θ1和θ2数值在一开始初始化是差不多的，但是由于X1和X2的关系，我们会发现θ1从初始值到目标位置的距离要远大于θ2从初始值到目标位置，这也是第二幅图扁平的原因"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每次调整θ1的幅度要远小于θ2的幅度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结：归一化的目的是使得最终梯度下降的时候可以不同维度θ参数可以在接近的调整幅度上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "扩展：归一化可以提高精度，主要在距离计算上，样本之间的数量级不同会导致偏向性，所以归一化后就没这个问题了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 最大值最小值归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min max scaling，针对每一列，每一列取出最大最小值，然后对这一列中每个数值按公式操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ X_{\\text{norm}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $X$ 是原始数据点。\n",
    "* $X_{min}$ 是数据集中的最小值。\n",
    "* $X_{max}$ 是数据集中的最大值。\n",
    "* $X_{norm}$ 是归一化后的数据点。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优点是一定可以把数值归一到0到1之间，缺点是如果有一个离群值，影响会比较大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 标准归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也是按照列来做的，求出每一列的均值和标准差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标准归一化包含了均值归一化和方差归一化，经过处理的函数符合标准正态分布，即均值为0，标准差为1，其转化函数为： \n",
    "$$\n",
    "X_{\\text{norm}} = \\frac{X - \\mu}{\\sigma}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方差归一化：标准差的计算会考虑到所有的样本数据，所以受到离群值的影响会小了一些。  \n",
    "均值归一化：当我们梯度下降的时候，能够有更多的方向可以调整"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据不一定会缩放到0到1之间了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [5],\n",
       "       [5]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "temp = np.array([1,2,3,5,5])\n",
    "temp = temp.reshape(-1,1)\n",
    "temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(temp) # fit 方法计算数据的均值和标准差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.56])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.var_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.375],\n",
       "       [-0.75 ],\n",
       "       [-0.125],\n",
       "       [ 1.125],\n",
       "       [ 1.125]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.transform(temp) # 对数据进行标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([1,2,3,5,50001]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    1],\n",
       "       [    2],\n",
       "       [    3],\n",
       "       [    5],\n",
       "       [50001]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10002.4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.99972002e+08])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.var_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5000875 ],\n",
       "       [-0.5000375 ],\n",
       "       [-0.4999875 ],\n",
       "       [-0.49988749],\n",
       "       [ 2.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 强调"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在做特征工程的时候，对训练集做归一化后，测试集也同样需要做归一化，但是它的均值和方差要使用训练集的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 正则化 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 过拟合和欠拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）under fit：还没有拟合到位，训练集和测试集的准确率都还没达到最高。学的还不到位。  \n",
    "（2）over fit：拟合过度，训练集的准确率升高的同时，测试集的准确率反而降低。学的过度了，做过的卷子都能再次答对，考试碰到新题就不会了   \n",
    "（3）just right：过拟合前训练集和测试集准确率都达到最高时刻。学习并不需要花费很多时间，理解的很好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Img/11.png\" width=\"40%\" height=\"40%\">   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正则化就是防止过拟合，具有更好的鲁棒性（robust），也就是模型的泛化能力和推广能力更强。想要有一定的容错率又要保证正确率就要由正则项来决定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正则化的本质就是牺牲模型在训练集上的正确率来提高推广能力，W在数值上越小越好，这样能抵抗数值的扰动。同时为了保证模型的正确率，W又不能极小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此在损失函数的后面加上一个惩罚项，使得计算出来的模型W相对小一些，带来泛化能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Loss = MSE + 正则项$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原来的损失函数是保证MSE最小，找到使得训练集正确率最高的参数。但是我们需要的是测试集上的准确率越高，因此MSE和正则项都得小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（回归的损失函数一般是MSE，分类的话一般是cross entropy交叉熵）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 常用的惩罚项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实L1和L2正则的公式数学里面的意义就是范数，代表空间中向量到原点的距离，W整体越小越好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### L1正则项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数距离原点的曼哈顿距离"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{i=1}^{n} |w_i|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在线性回归中，包含L1正则项的损失函数可以是：\n",
    "$$\\frac{1}{2m} \\sum_{j=1}^{m} (y_j - \\hat{y_j})^2 + \\lambda \\sum_{i=1}^{n} |w_i|$$\n",
    "其中$ \\lambda $是正则化系数，控制惩罚项的权重，m表示样本数量  \n",
    "这个就是Lasso回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.85696912]\n",
      "[5.05414109]\n",
      "[2.53521868]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "x = 2*np.random.rand(100,1)\n",
    "y = 4 + 3*x + np.random.rand(100,1)\n",
    "\n",
    "# alpha 是正则化强度的超参数，用于控制 L2 正则化项的权重\n",
    "# max_iter 是最大迭代次数\n",
    "# alpha调大会更关注泛化能力，会牺牲一些准确率\n",
    "ridge_reg = Lasso(alpha=0.15,max_iter=30000)\n",
    "ridge_reg.fit(x,y)\n",
    "print(ridge_reg.predict([[1.5]]))\n",
    "\n",
    "# 打印模型的截距和系数\n",
    "print(ridge_reg.intercept_)\n",
    "print(ridge_reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### L2正则项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数距离原点的欧式距离"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{i=1}^{n} w_i^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在线性回归中，包含L2正则项的损失函数可以是：\n",
    "$$\\frac{1}{2m} \\sum_{j=1}^{m} (y_j - \\hat{y_j})^2 + \\frac{\\lambda}{2} \\sum_{i=1}^{n} w_i^2$$\n",
    "这个就是Ridge岭回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.90842188]]\n",
      "[4.60162269]\n",
      "[[2.87119946]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "x = 2*np.random.rand(100,1)\n",
    "y = 4 + 3*x + np.random.rand(100,1)\n",
    "\n",
    "# alpha 是正则化强度的超参数，用于控制 L2 正则化项的权重\n",
    "# solver 参数指定了用于优化的算法。sag'：使用随机平均梯度下降法\n",
    "# alpha调大会更关注泛化能力，会牺牲一些准确率\n",
    "ridge_reg = Ridge(alpha=0.0004,solver='sag')\n",
    "ridge_reg.fit(x,y)\n",
    "print(ridge_reg.predict([[1.5]]))\n",
    "\n",
    "# 打印模型的截距和系数\n",
    "print(ridge_reg.intercept_)\n",
    "print(ridge_reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### L1稀疏L2平滑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1正则会使得计算出来的模型有的W趋近于0，有的W相对较大，而L2会使得W参数整体变小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到菱形的交点更多可能在坐标轴上，会导致一些参数值为0，这也是为什么L1稀疏的原因"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Img/12.png\" width=\"40%\" height=\"40%\">   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事实上，我们使用了新的损失函数后，我们的目标不在是原来的照趋近于圆心的点，而是变成了找两个图形的交点，找出哪个满足总损失函数最小的那个交点就是我们想要的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ElasticNet回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ ElasticNet 损失函数 = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( y_i - \\hat{y_i} \\right)^2 + \\alpha \\left( \\frac{1 - \\text{l1\\_ratio}}{2} \\sum_{j=1}^{n} w_j^2 + \\text{l1\\_ratio} \\sum_{j=1}^{n} |w_j| \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "α 是正则化强度的超参数。  \n",
    "l1_ratio 是 L1 正则项的权重比率，介于 0 和 1 之间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.76029894]\n",
      "[5.1218699]\n",
      "[2.42561936]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "x = 2*np.random.rand(100,1)\n",
    "y = 4 + 3*x + np.random.rand(100,1)\n",
    "\n",
    "# alpha 是正则化强度的超参数，用于控制 L2 正则化项的权重\n",
    "# solver 参数指定了用于优化的算法。sag'：使用随机平均梯度下降法\n",
    "# alpha调大会更关注泛化能力，会牺牲一些准确率\n",
    "ridge_reg = ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=10000)\n",
    "ridge_reg.fit(x,y)\n",
    "print(ridge_reg.predict([[1.5]]))\n",
    "\n",
    "# 打印模型的截距和系数\n",
    "print(ridge_reg.intercept_)\n",
    "print(ridge_reg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Opencv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
