{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最古老的分类算法之一，泛化能力弱。思想是在任意空间中，感知器模型寻找一个超平面，把所有的二元类别分隔开，他的前提是数据是线性可分的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于m个样本，每个样本n维特征以及一个二元类别输出y，目标是找到一个超平面：\n",
    "$$\\theta_0+\\theta_1x_1+\\cdots+\\theta_nx_n=0$$\n",
    "也就是$\\theta^T x=0$，让一个类别的样本满足：$\\theta^T x > 0$；另外一个类别的满足：$\\theta^T x <0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感知器的模型：\n",
    "$\\hat{y} = sign(\\theta^Tx)=\\begin{cases}\n",
    "+1, & \\theta^Tx > 0 \\\\\n",
    "-1, & \\theta^Tx < 0\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正确分类：$y*\\theta x>0$，错误分类：$y*\\theta x<0$；所以我们可以定义我们的损失函数为：期望使分类错误的所有样本到超平面的距离之和最小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逻辑回归的几何意义，也是在向量空间找到一个超平面，超平面一侧的点计算分数结果为负，另一侧结果分数为正，只不过最后不直接看sign符合，而是根据sigmoid函数将分数映射到0-1之间通过最大似然来赋予概率意义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 感知器的损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上面的损失函数的定义我们能得出：\n",
    "$$L = -\\sum_{i=1}^{m}\\frac{y_i\\theta*x_i}{||\\theta||_2}$$\n",
    "$\\theta$分子分母之间存在倍数，故可以损失函数简化为：\n",
    "$$L = -\\sum_{i=1}^{m}y_i\\theta*x_i$$\n",
    "一般使用SGD（随机梯度下降）来求解：\n",
    "$$\\theta^{k+1}=\\theta^k+\\alpha y_ix_i$$\n",
    "$\\alpha$是学习率，这样就可以完成了参数的更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM(支持向量机)算法思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM也是通过寻找超平面，用于解决二分类问题的分类算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与感知器相同，通过sign给出预测标签，正例为+1，负例为-1，模型判别式同样：\n",
    "$$\\hat{y} = sign(\\theta^Tx)=\\begin{cases}\n",
    "+1, & \\theta^Tx > 0 \\\\\n",
    "-1, & \\theta^Tx < 0\n",
    "\\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感知机是通过判错的点寻找超平面，逻辑回归通过最大似然寻找超平面，SVM是通过支持向量寻找超平面，这也是损失函数不同的原因"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "找到离超平面最近的点，越远越好，那么这个点就是支持向量。（有条件的二次优化问题）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "间隔（Margin）：数据点到分割超平面的距离称为间隔   \n",
    "支持向量（Support Vector）：离分割超平面最近的那些点叫做支持向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM的损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$min \\frac{1}{2}||w||_2^2  $$\n",
    "$$s.t y_i(w^Tx_i+b) \\geq 1(i=1,2,\\cdots m)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "E_opencv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
