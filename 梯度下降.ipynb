{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "是一个通用的优化算法来帮助机器学习算法求解出最优解的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有优化算法的目的都是以最快的速度把模型参数θ求解出来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前利用解析解公式一步求出参数的最优解，它的复杂度可以达到N的3次方，并不是常用的手段，而梯度下降法是一点点逼近最优解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Img/6.png\" width=\"40%\" height=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个曲线其实就是损失函数，直到我们得到的MSE最小的时候，计算出来的θ就是我们的最优解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结出来的公式：\n",
    "$$\n",
    "W_j^{t+1} = W_j^t - \\eta * gradient_j\n",
    "$$\n",
    "对于$\\eta$值来说，它叫学习率，必定为正，所以当梯度为正的时候，W会往小的方向调整，梯度为负的时候，W会往大的方向调整"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Img/7.png\" width=\"50%\" height=\"50%\">   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果损失函数是非凸函数，梯度下降法最有可能落到局部最小值的，所以其实步长不能设置太小，那样会容易落入局部最优解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习大部分损失函数是凸函数，而Deep learning 是非凸函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 梯度下降法流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度下降法几步流程就是猜正确答案的过程：  \n",
    "* 瞎蒙，Random随机θ，随机一组数值$W_0...W_n$\n",
    "* 求梯度，延切线方向往下下降就相当于沿着坡度最陡峭的方向下降\n",
    "* if g < 0, θ变大，if g > 0, θ变小\n",
    "* 判断是否收敛convergence，如果收敛跳出迭代，如果没有收敛，回第2步继续"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Img/8.png\" width=\"50%\" height=\"50%\">   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "怎么求梯度？  \n",
    "* $gradient_j = \\frac{ \\partial Loss }{ \\partial w_j }$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何调整θ或W？ \n",
    "* $W_j^{t+1} = W_j^t - \\eta * gradient_j $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "怎么判断收敛？\n",
    "* 判断收敛这里使用g=0其实并不合理，因为当损失函数是非凸函数的话，g=0有可能是极大值点，所以我们判断loss的下降收益更合理，当随着迭代loss减少的幅度即收益不再变化就可以认为停止在最低点，收敛！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
