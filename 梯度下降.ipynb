{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "是一个通用的优化算法来帮助机器学习算法求解出最优解的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有优化算法的目的都是以最快的速度把模型参数θ求解出来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前利用解析解公式一步求出参数的最优解，它的复杂度可以达到N的3次方，并不是常用的手段，而梯度下降法是一点点逼近最优解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Img/6.png\" width=\"40%\" height=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个曲线其实就是损失函数，直到我们得到的MSE最小的时候，计算出来的θ就是我们的最优解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结出来的公式：\n",
    "$$\n",
    "W_j^{t+1} = W_j^t - \\eta * gradient_j\n",
    "$$\n",
    "对于$\\eta$值来说，它叫学习率，必定为正，所以当梯度为正的时候，W会往小的方向调整，梯度为负的时候，W会往大的方向调整"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Img/7.png\" width=\"50%\" height=\"50%\">   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果损失函数是非凸函数，梯度下降法最有可能落到局部最小值的，所以其实步长不能设置太小，那样会容易落入局部最优解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习大部分损失函数是凸函数，而Deep learning 是非凸函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 梯度下降法流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度下降法几步流程就是猜正确答案的过程：  \n",
    "* 瞎蒙，Random随机θ，随机一组数值$W_0...W_n$\n",
    "* 求梯度，延切线方向往下下降就相当于沿着坡度最陡峭的方向下降\n",
    "* if g < 0, θ变大，if g > 0, θ变小\n",
    "* 判断是否收敛convergence，如果收敛跳出迭代，如果没有收敛，回第2步继续"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Img/8.png\" width=\"50%\" height=\"50%\">   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "怎么求梯度？  \n",
    "* $gradient_j = \\frac{ \\partial Loss }{ \\partial w_j }$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何调整θ或W？ \n",
    "* $W_j^{t+1} = W_j^t - \\eta * gradient_j $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "怎么判断收敛？\n",
    "* 判断收敛这里使用g=0其实并不合理，因为当损失函数是非凸函数的话，g=0有可能是极大值点，所以我们判断loss的下降收益更合理，当随着迭代loss减少的幅度即收益不再变化就可以认为停止在最低点，收敛！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 三种梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "区别在于第2步求梯度所用到的X的数据集的样本数量不同！它们每次学习（更新模型参数）使用的样本个数，每次更新使用不同的样本会导致每次学习的准确性和学习时间不同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Img/9.png\" width=\"20%\" height=\"20%\">   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 全量梯度下降（Batch Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在梯度下降中，对于θ的更新，所有的样本都有贡献，也就是参与调整θ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一次更新的幅度是比较大的，样本不多的情况，收敛速度快"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每次学习时间过长，若训练集过大，会消耗大量的内存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每次更新都会朝着正确的方向进行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.84592664],\n",
       "       [3.40610881]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 创建数据集x,y\n",
    "x = np.random.rand(100,1)\n",
    "y = 4 + 3*x + np.random.randn(100,1)\n",
    "x_b = np.c_[np.ones((100,1)),x]\n",
    "\n",
    "# 创建超参数\n",
    "n_iterations = 10000 # 迭代次数\n",
    "\n",
    "t0,t1 = 5, 500\n",
    "\n",
    "# 定义一个函数来调整学习率，学习率不断减小\n",
    "def learning_rate_schedule(t):\n",
    "    return t0/(t+t1)\n",
    "\n",
    "# 1. 初始化θ，W0..Wn , 标准正态分布创建W\n",
    "θ = np.random.randn(2,1)\n",
    "\n",
    "# 4. 判断是否收敛，一般不会去设定阈值，而是直接采用设置相对大的迭代次数保证可以收敛\n",
    "for i in range(n_iterations):\n",
    "    # 2. 求梯度，计算gradient\n",
    "    gradients = x_b.T.dot(x_b.dot(θ)-y)\n",
    "    # 3. 应用梯度下降法的公式去调整θ值，θt+1 = θt - η*gradient\n",
    "    learning_rate = learning_rate_schedule(i)\n",
    "    θ = θ - learning_rate * gradients\n",
    "\n",
    "θ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. 随机梯度下降（Stochastic Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每次从训练集中随机选择一个样本来学习，故每次学习的速度比较快"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每次更新可能并不会按照正确的方向进行，会带来优化波动"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "会导致迭代次数的增加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.14629472],\n",
       "       [2.75018676]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 创建数据集x,y\n",
    "x = np.random.rand(100,1)\n",
    "y = 4 + 3*x + np.random.randn(100,1)\n",
    "x_b = np.c_[np.ones((100,1)),x]\n",
    "\n",
    "# 创建超参数\n",
    "n_epochs = 10000\n",
    "m = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# 1. 初始化θ，W0..Wn , 标准正态分布创建W\n",
    "θ = np.random.randn(2,1)\n",
    "\n",
    "# 4. 判断是否收敛，一般不会去设定阈值，而是直接采用设置相对大的迭代次数保证可以收敛\n",
    "for epoch in range(n_epochs):\n",
    "    # 在双层for循环之间，每个轮次开始分批次迭代之前打乱数据索引顺序\n",
    "    arr = np.arange(len(x_b))\n",
    "    np.random.shuffle(arr)\n",
    "    x_b = x_b[arr]\n",
    "    y = y[arr]\n",
    "    for i in range(m):\n",
    "        x_i = x_b[i:i+1]\n",
    "        y_i = y[i:i+1]\n",
    "        # 2. 求梯度，计算gradient\n",
    "        gradients = x_i.T.dot(x_i.dot(θ)-y_i)\n",
    "        # 3. 应用梯度下降法的公式去调整θ值，θt+1 = θt - η*gradient\n",
    "        θ = θ - learning_rate * gradients\n",
    "\n",
    "θ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. 小批量梯度下降（Mini-Batch Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每次更新随机选择batch个样本进行学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "降低了收敛波动性，更新更稳定，选择一个更新速度和更新次数比较适合的样本数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch的范围[50，256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.14341932],\n",
       "       [2.99016574]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 创建数据集x,y\n",
    "x = np.random.rand(100,1)\n",
    "y = 4 + 3*x + np.random.randn(100,1)\n",
    "x_b = np.c_[np.ones((100,1)),x]\n",
    "\n",
    "# 创建超参数\n",
    "n_epochs = 10000\n",
    "m = 100\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_batches = int(m / batch_size)\n",
    "\n",
    "# 1. 初始化θ，W0..Wn , 标准正态分布创建W\n",
    "θ = np.random.randn(2,1)\n",
    "\n",
    "# 4. 判断是否收敛，一般不会去设定阈值，而是直接采用设置相对大的迭代次数保证可以收敛\n",
    "for epoch in range(n_epochs):\n",
    "    # 读取x_b的索引0-99\n",
    "    arr = np.arange(len(x_b))\n",
    "    np.random.shuffle(arr)\n",
    "    x_b = x_b[arr]\n",
    "    y = y[arr]\n",
    "    for i in range(num_batches):\n",
    "        \n",
    "        x_batch = x_b[i*batch_size : i*batch_size + batch_size]\n",
    "        y_batch = y[i*batch_size : i*batch_size + batch_size]\n",
    "        # 2. 求梯度，计算gradient\n",
    "        gradients = x_batch.T.dot(x_batch.dot(θ)-y_batch)\n",
    "        # 3. 应用梯度下降法的公式去调整θ值，θt+1 = θt - η*gradient\n",
    "        θ = θ - learning_rate * gradients\n",
    "\n",
    "θ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "E_opencv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
