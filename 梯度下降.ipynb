{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "是一个通用的优化算法来帮助机器学习算法求解出最优解的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有优化算法的目的都是以最快的速度把模型参数θ求解出来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前利用解析解公式一步求出参数的最优解，它的复杂度可以达到N的3次方，并不是常用的手段，而梯度下降法是一点点逼近最优解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Img/6.png\" width=\"40%\" height=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个曲线其实就是损失函数，直到我们得到的MSE最小的时候，计算出来的θ就是我们的最优解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结出来的公式：\n",
    "$$\n",
    "W_j^{t+1} = W_j^t - \\eta * gradient_j\n",
    "$$\n",
    "对于$\\eta$值来说，它叫学习率，必定为正，所以当梯度为正的时候，W会往小的方向调整，梯度为负的时候，W会往大的方向调整"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Img/7.png\" width=\"50%\" height=\"50%\">   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果损失函数是非凸函数，梯度下降法最有可能落到局部最小值的，所以其实步长不能设置太小，那样会容易落入局部最优解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习大部分损失函数是凸函数，而Deep learning 是非凸函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
